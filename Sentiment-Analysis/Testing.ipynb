{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wXmcy4u08KXI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ANALISIS SENTIMEN PENGGUNA TERHADAP APLIKASI BRIMO BRI DI PLAY STORE**"
      ],
      "metadata": {
        "id": "w1kNI5KJ8gvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Testing**"
      ],
      "metadata": {
        "id": "YPTTZ2XM8ixX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Preparation"
      ],
      "metadata": {
        "id": "wXmcy4u08KXI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsXV4t8Y3FpF",
        "outputId": "b75858a6-6d46-4d61-ff03-107cd732be01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install Sastrawi\n",
        "!pip install nltk\n",
        "import pickle\n",
        "from pickle import UnpicklingError\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import requests\n",
        "import json\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Muat model SVM\n",
        "try:\n",
        "    with open('svm_model.pkl', 'rb') as f:\n",
        "        svm = pickle.load(f)\n",
        "except (EOFError, UnpicklingError) as e:\n",
        "    print(f\"Error loading SVM model: {e}\")\n",
        "    # Additional diagnostic steps:\n",
        "    print(\"Check if 'svm_model.pkl' exists and is not empty.\")\n",
        "    print(\"If the file was transferred, ensure it was done in binary mode.\")\n",
        "\n",
        "# Muat TfidfVectorizer\n",
        "try:\n",
        "    with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
        "        tfidf = pickle.load(f)\n",
        "except (EOFError, UnpicklingError) as e:\n",
        "    print(f\"Error loading TF-IDF vectorizer: {e}\")\n",
        "\n",
        "# Fungsi pra-pemrosesan teks (sama seperti di Pemodelan.ipynb)\n",
        "def cleaningText(text):\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Menghapus mention (@username)\n",
        "    text = re.sub(r'#[A-Za-z0-9]+', '', text)  # Menghapus hashtag (#hashtag)\n",
        "    text = re.sub(r\"http\\S+\", '', text)  # Menghapus URL\n",
        "    text = re.sub(r'[0-9]+', '', text)  # Menghapus angka\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Menghapus karakter non-alfanumerik kecuali spasi\n",
        "    text = text.replace('\\n', ' ')  # Mengganti newline dengan spasi\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Menghapus tanda baca\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in [\"brimo\", \"bri\", \"bri mobile\"]])  # Menghapus kata-kata yang diinginkan\n",
        "    text = text.strip(' ')  # Menghapus spasi di awal dan akhir teks\n",
        "    return text  # Mengembalikan teks yang telah dibersihkan\n",
        "\n",
        "def casefoldingText(text):\n",
        "    text = text.lower()  # Mengubah semua karakter dalam teks menjadi huruf kecil\n",
        "    return text  # Mengembalikan teks yang telah di-casefold\n",
        "\n",
        "def tokenizingText(text):\n",
        "    text = word_tokenize(text)  # Memecah teks menjadi token (kata-kata)\n",
        "    return text  # Mengembalikan daftar token\n",
        "\n",
        "def filteringText(text):\n",
        "    listStopwords = set(stopwords.words('indonesian'))  # Mengambil daftar stopwords bahasa Indonesia\n",
        "    listStopwords1 = set(stopwords.words('english'))  # Mengambil daftar stopwords bahasa Inggris\n",
        "    listStopwords.update(listStopwords1)  # Menggabungkan dua daftar stopwords\n",
        "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku','di','ya','loh','kah','deh'])\n",
        "    filtered = []\n",
        "    for txt in text:\n",
        "        if txt not in listStopwords:  # Memeriksa apakah kata bukan stopword\n",
        "            filtered.append(txt)  # Menambahkan kata yang bukan stopword ke daftar filtered\n",
        "    text = filtered  # Menggantikan teks dengan daftar kata yang telah difilter\n",
        "    return text  # Mengembalikan daftar kata yang telah difilter\n",
        "\n",
        "def stemmingText(text):\n",
        "    factory = StemmerFactory()  # Membuat pabrik stemmer\n",
        "    stemmer = factory.create_stemmer()  # Membuat stemmer\n",
        "    words = text.split()  # Memecah teks menjadi kata-kata\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]  # Melakukan stemming pada setiap kata\n",
        "    stemmed_text = ' '.join(stemmed_words)  # Menggabungkan kata-kata yang telah distem menjadi satu kalimat\n",
        "    return stemmed_text  # Mengembalikan teks yang telah distem\n",
        "\n",
        "def toSentence(list_words):\n",
        "    sentence = ' '.join(word for word in list_words)  # Menggabungkan daftar kata menjadi satu kalimat\n",
        "    return sentence  # Mengembalikan kalimat yang dibentuk dari daftar kata\n",
        "\n",
        "def fix_slangwords(text):\n",
        "    words = text.split()  # Memecah teks menjadi kata-kata\n",
        "    fixed_words = []  # Inisialisasi daftar untuk kata-kata yang sudah diperbaiki\n",
        "    for word in words:\n",
        "        if word.lower() in slangwords:  # Memeriksa apakah kata tersebut adalah kata slang\n",
        "            fixed_words.append(slangwords[word.lower()])  # Jika ya, mengganti kata slang dengan kata standar\n",
        "        else:\n",
        "            fixed_words.append(word)  # Jika bukan kata slang atau kata yang ingin dihapus, tetap mempertahankan kata tersebut\n",
        "    fixed_text = ' '.join(fixed_words)  # Menggabungkan kata-kata yang sudah diperbaiki menjadi satu teks\n",
        "    return fixed_text  # Mengembalikan teks yang sudah diperbaiki\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/mashumabduljabbar/dataset/master/slangwords.json'  # URL tempat kamus slangwords disimpan\n",
        "\n",
        "response = requests.get(url)  # Mengirim permintaan HTTP GET ke URL untuk mendapatkan konten\n",
        "\n",
        "if response.status_code == 200:  # Jika status code respons adalah 200 (OK)\n",
        "    try:\n",
        "        slangwords = json.loads(response.text)  # Mengurai JSON yang diperoleh dari respons dan menyimpannya dalam variabel slangwords\n",
        "    except json.JSONDecodeError as e: # Menangani kesalahan jika terjadi masalah saat mengurai JSON\n",
        "        print(\"Error decoding JSON:\", e)\n",
        "        print(\"Response content:\", response.text)\n",
        "else:\n",
        "    print(\"Failed to fetch data from URL. Status code:\", response.status_code) # Menampilkan pesan kesalahan jika tidak berhasil mengambil data dari URL\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = cleaningText(text)\n",
        "    text = casefoldingText(text)\n",
        "    text = fix_slangwords(text)\n",
        "    text = tokenizingText(text)\n",
        "    text = filteringText(text)\n",
        "    text = toSentence(text)\n",
        "    return text\n",
        "\n",
        "def prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm):\n",
        "    # Melakukan preprocessing pada kalimat baru\n",
        "    review_baru_cleaned = cleaningText(review_baru)\n",
        "    review_baru_casefolded = casefoldingText(review_baru_cleaned)\n",
        "    review_baru_slangfixed = fix_slangwords(review_baru_casefolded)\n",
        "    review_baru_tokenized = tokenizingText(review_baru_slangfixed)\n",
        "    review_baru_filtered = filteringText(review_baru_tokenized)\n",
        "    review_baru_final = toSentence(review_baru_filtered)\n",
        "\n",
        "    # Menggunakan objek tfidf yang sudah di-fit dari pelatihan sebelumnya\n",
        "    X_review_baru = tfidf.transform([review_baru_final])\n",
        "\n",
        "    # Convert sparse matrix to dense array\n",
        "    X_review_baru = X_review_baru.toarray()\n",
        "\n",
        "    # Memperoleh prediksi sentimen review baru menggunakan model terbaik\n",
        "    prediksi_sentimen = svm.predict(X_review_baru)\n",
        "\n",
        "    # Menampilkan hasil prediksi\n",
        "    if prediksi_sentimen[0] == 'positive':\n",
        "        hasil = \"Sentimen review baru adalah POSITIF.\"\n",
        "    elif prediksi_sentimen[0] == 'negative':\n",
        "        hasil = \"Sentimen review baru adalah NEGATIF.\"\n",
        "    else:\n",
        "        hasil = \"Sentimen review baru adalah NETRAL.\"\n",
        "\n",
        "    return hasil"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Kategori Negatif"
      ],
      "metadata": {
        "id": "nH-FeiXh8ZlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_baru = \"susah banget loginnya nih apps gagal mulu\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "y5IaqTYs4Vwq",
        "outputId": "5bf318cc-5af8-4064-f9d0-8fe303570372"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah NEGATIF.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Kategori Positif"
      ],
      "metadata": {
        "id": "1UcN2RIz8_gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_baru = \"Aplikasi yg bermanfaat, membantu transaksi saya tidak pernah gagal\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "E6dphSXG7_Tl",
        "outputId": "2833406a-4780-400e-c916-f7f2c569870b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah POSITIF.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Kategori Netral"
      ],
      "metadata": {
        "id": "phu5pG5D9CUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_baru = \"Sudah oke\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GXDW4L--8DM2",
        "outputId": "6d3569a0-426e-4ffc-fba4-a7cf3542981a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah NETRAL.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}